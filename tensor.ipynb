{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# tensor初始化、数学操作等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`torch.tensor()`将numpy或者list等转换为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "my_tensor=torch.tensor(np.ones(3))\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor=torch.tensor([1,2,3])\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用多维list初始化多维tensor,例如下述代码初始化一个两行三列的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.tensor([[1,2,3],[4, 5, 6]])\n",
    "print(my_tenosr)\n",
    "print(my_tenosr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`dtype`字段指定数据类型，`dtype`可选`torch.float32`, `torch.float64`, `torch.float16`, `torch.int8`, `torch.int16`, `torch.int32`等\n",
    "\n",
    "例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor=torch.tensor([[1,2,3],[4, 5, 6]],dtype=torch.float64)\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor=torch.tensor([[1,2,3],[4, 5, 6]],dtype=torch.float32)\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor=torch.tensor([[1,2,3],[4, 5, 6]],dtype=torch.int32)\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过`device`字段指定将tensor分配分配给哪个设备（cpu或则gpu）。具体来说，`torch.device`代表将`torch.Tensor`分配到的设备的对象。`torch.device`包含一个设备类型（‘cpu’或‘cuda’）和可选的设备序号。如果设备序号不存在，则为当前设备。如：`torch.Tensor`用设备构建`'cuda'`的结果等同于`‘cuda：X’`，其中X是`torch.cuda.current_device()`的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.tensor([[1,2,3],[4, 5, 6]],dtype=torch.int32,device='cpu')\n",
    "print(my_tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用如下代码，优先将tensor分配给GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "mydevice='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(mydevice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.tensor([1,2,3],device=mydevice)\n",
    "print(my_tensor.device)\n",
    "print(my_tensor.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过字段'requires_grad'来指定tensor是否需要进行求导（反向传播），默认情况下该字段为`False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(my_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.tensor([1.5,2,3],requires_grad=True)\n",
    "print(my_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果想改变`requires_grad`这个属性，可以调用`tensor.requires_grad_()`方法：\n",
    "```\n",
    "x.requires_grad_(True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.tensor([1.5,2,3])\n",
    "print(my_tensor.requires_grad)\n",
    "my_tensor.requires_grad_(True)\n",
    "print(my_tensor.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填充指定size的tensor\n",
    "1. torch.empty()  \n",
    "2. torch.ones()\n",
    "3. torch.zeros()  \n",
    "4. torch.eye()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. torch.empty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`torch.empty()`可以创建包含未初始化数据的张量\n",
    "标量值0填充的张量,其形状由变量参数size定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.empty((3,4))\n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)#具体值实际上待定，不要以为是初始化为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. torch.zeros()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.zeros(3,4)\n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. torch.ones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.ones(3,4,5)\n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. torch.eye()  \n",
    "生成对角线全1，其余部分全0的二维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.eye(3)\n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.eye(2,3)\n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. torch.rand()  \n",
    "包含了从区间[0, 1)的均匀分布中抽取的一组随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([[0.4518, 0.6142, 0.7612, 0.3828],\n",
      "        [0.6862, 0.0493, 0.5492, 0.2327],\n",
      "        [0.7688, 0.1837, 0.6124, 0.3847]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.rand(3,4)\n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. torch.randn()  \n",
    "包含了从**标准正态分布**中抽取的一组随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([[ 0.0647, -0.1434,  2.0323, -0.2759],\n",
      "        [-0.9258,  0.3505, -0.7455, -0.0367],\n",
      "        [ 0.6792,  0.5661,  1.5247, -1.3066]])\n"
     ]
    }
   ],
   "source": [
    "my_tensor=torch.randn(3,4)\n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.arange( start, end, step)：不含end (建议使用)\n",
    "#### torch.range( start, end, step): 含end (不建议使用，后续可能会被移除)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "torch.Size([6])\n",
      "tensor([0., 1., 2., 3., 4., 5.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-82-c200653eb586>:4: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  my_tensor = torch.range(start=0, end=5, step=1)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.arange(start=0, end=5, step=1)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)\n",
    "my_tensor = torch.range(start=0, end=5, step=1)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以不用加start和end关键字，按参数次序写即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([0, 2, 4])\n",
      "torch.Size([3])\n",
      "tensor([0., 2., 4.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-88-1464d512cadc>:4: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  my_tensor = torch.range(0,5,2)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.arange(0,5,2)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)\n",
    "my_tensor = torch.range(0,5,2)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step参数默认为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "torch.Size([6])\n",
      "tensor([0., 1., 2., 3., 4., 5.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-84-5b1a9771b9c9>:4: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  my_tensor = torch.range(0,5)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.arange(0,5)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)\n",
    "my_tensor = torch.range(0,5) #注意，结果中包含end \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以只传入end参数，此时默认start为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-80-15233ef67f78>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-80-15233ef67f78>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    my_tensor = torch.arange( ,11,2)\u001b[0m\n\u001b[1;37m                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.arange( ,11,2)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "torch.Size([6])\n",
      "tensor([0., 1., 2., 3., 4., 5.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-87-1bff08bca9ba>:4: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  my_tensor = torch.range(0,5)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.arange(5)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)\n",
    "my_tensor = torch.range(0,5)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(start=0.1, end=1, steps=10)  # x = [0.1, 0.2, ..., 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.linspace(start, end, steps)\n",
    "`torch.linspace`同`torch.range`均是左闭右闭，即返回值中既包含start也包含end. 但是不同之处在于，`torch.linspace`中的steps参数应该理解为元素总个数，而`torch.range`中的step参数应该理解为步长（前后元素之间的差）\n",
    "这个张量包含了从start到end（包括端点）的等距的steps个数据点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([0., 2., 4.])\n",
      "torch.Size([2])\n",
      "tensor([0., 5.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-89-1f46291fd614>:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  my_tensor = torch.range(0,5,2)\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.range(0,5,2)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)\n",
    "my_tensor = torch.linspace(0,5,2)  \n",
    "print(my_tensor.shape)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.diag(input, diagonal=0, out=None) → Tensor\n",
    "如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵\n",
    "如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量\n",
    "\n",
    "参数diagonal指定对角线:\n",
    "diagonal = 0, 主对角线\n",
    "diagonal > 0, 主对角线之上\n",
    "diagonal < 0, 主对角线之下\n",
    "————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.ones(3,3)\n",
    "print(torch.diag(my_tensor))#主对角线\n",
    "print(torch.diag(my_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.diag(my_tensor,1)) #主对角线之上的第一条对角线\n",
    "print(torch.diag(my_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.diag(my_tensor,2)) #主对角线之上的第二条对角线\n",
    "print(torch.diag(my_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.diag(my_tensor,-2)) #主对角线之下的第二条对角线\n",
    "print(torch.diag(my_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(torch.diag(my_tensor,3)) #主对角线之上的第三条对角线（无元素）\n",
    "print(torch.diag(my_tensor).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换数值类型(int, float, double)等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2, -1,  0,  1,  2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor = torch.tensor([-2,-1,0,1,2])\n",
    "my_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为布尔类型（将非0值转为True）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Boolean: tensor([ True,  True, False,  True,  True])\n",
      "Converted int16 tensor([-2, -1,  0,  1,  2], dtype=torch.int16)\n",
      "Converted int64 tensor([-2, -1,  0,  1,  2])\n",
      "Converted float16 tensor([-2., -1.,  0.,  1.,  2.], dtype=torch.float16)\n",
      "Converted float32 tensor([-2., -1.,  0.,  1.,  2.])\n",
      "Converted float64 tensor([-2., -1.,  0.,  1.,  2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Converted Boolean: {my_tensor.bool()}\")  # Converted to Boolean: 1 if nonzero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted int16 tensor([-2, -1,  0,  1,  2], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Converted int16 {my_tensor.short()}\")  # Converted to int16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted int64 tensor([-2, -1,  0,  1,  2])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Converted int64 {my_tensor.long()}\"\n",
    ")  # Converted to int64 (This one is very important, used super often)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted float16 tensor([-2., -1.,  0.,  1.,  2.], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Converted float16 {my_tensor.half()}\")  # Converted to float16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted float32 tensor([-2., -1.,  0.,  1.,  2.])\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Converted float32 {my_tensor.float()}\"\n",
    ")  # Converted to float32 (This one is very important, used super often)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "转换为float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted float64 tensor([-2., -1.,  0.,  1.,  2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Converted float64 {my_tensor.double()}\")  # Converted to float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### torch.from_numpy()与torch.numpy()\n",
    "前者将numpy类型转换为tensor，后者将tensor转换为numpy类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], dtype=torch.float64)\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "np_array = np.zeros((5, 5))\n",
    "my_tensor = torch.from_numpy(np_array)\n",
    "np_array_again = (\n",
    "    my_tensor.numpy()\n",
    ")  # np_array_again will be same as np_array (perhaps with numerical round offs)\n",
    "print(my_tensor)\n",
    "print(np_array_again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor math\n",
    "### 加法：torch.add()，+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3., 4.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x=torch.tensor([1,2,3])\n",
    "y=torch.tensor([1,1,1])\n",
    "z=torch.empty(3)\n",
    "torch.add(x,y,out=z)\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y #简介明了，推荐使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "z1=torch.add(x,y)\n",
    "print(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 减法：torch.sub(),-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2,3])\n",
    "y=torch.tensor([1,1,1])\n",
    "z=torch.empty(3)\n",
    "torch.sub(x,y,out=z)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x-y #简介明了，推荐使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 哈达玛积(element wise，对应元素相乘)：torch.mul(), *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2,3])\n",
    "y=torch.tensor([1,1,1])\n",
    "z=torch.empty(3)\n",
    "torch.mul(x,y,out=z)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "哈达玛积运算中，x与y的维度（shape）必须一致。不一致的时候，若符合自动填充机制，则会进行自动填充（Broadcasting机制）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [1, 2, 3],\n",
      "        [1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.tensor([1,2,3])\n",
    "y=torch.tensor([[1],[1],[1]])\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print((x*y).shape)\n",
    "print((x*y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵乘法：\n",
    "二维矩阵乘法二维矩阵乘法运算操作包括torch.mm()、torch.matmul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 1)\n",
    "y = torch.ones(1, 2)\n",
    "print(torch.mm(a, b).shape)\n",
    "print(torch.matmul(a, b).shape)\n",
    "print((a @ b).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比二维矩阵中*与@的区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor([1,2,3])\n",
    "y=torch.tensor([[1],[1],[1]])\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print((x*y).shape)\n",
    "print((x@y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于高维的Tensor（dim>2），定义其矩阵乘法仅在最后的两个维度上(后面部分进行矩阵乘法)，要求前面的维度必须保持一致(前面部分进行哈达玛积)，操只有torch.matmul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4, 5)\n",
    "y = torch.rand(2, 3, 5, 4)\n",
    "print(torch.matmul(x, y).shape)\n",
    "#print(x)\n",
    "#print(y)\n",
    "#print(torch.matmul(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于前面部分是进行哈达玛积（具有Broadcasting机制），因而高维矩阵相乘时，若前面的\"矩阵索引维度\"如果符合Broadcasting机制，也会自动做广播，然后相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4, 5)\n",
    "y = torch.rand(2, 1, 5, 4)\n",
    "print(torch.matmul(x, y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4, 5)\n",
    "y = torch.rand(1, 3, 5, 4)\n",
    "print(torch.matmul(x, y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4, 5)\n",
    "y = torch.rand(3, 5, 4)\n",
    "print(torch.matmul(x, y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4, 5)\n",
    "y = torch.rand(1, 5, 4)\n",
    "print(torch.matmul(x, y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 幂运算: torch.pow(), **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  9, 16, 25])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([2, 3, 4, 5])\n",
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  9, 16, 25])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(x,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4,  9, 16, 25])\n",
      "tensor([2, 3, 4, 5])\n",
      "tensor([ 4,  9, 16, 25])\n",
      "tensor([ 4,  9, 16, 25])\n"
     ]
    }
   ],
   "source": [
    "print(x.pow(2))  \n",
    "print(x)         # 前述运算都不会改变x的值\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inplace操作符：原操作符后跟_，例如add_,pow_等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 16,  81, 256, 625])\n",
      "tensor([ 16,  81, 256, 625])\n"
     ]
    }
   ],
   "source": [
    "print(x.pow_(2)) #改变x的值\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 除法：torch.div(), /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5])\n",
      "torch.Size([2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4, 5)\n",
    "y = torch.rand(2, 3, 4, 5)\n",
    "print((x/y).shape)\n",
    "print(torch.div(x, y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 指数运算\n",
    "$y_i=e^{(x_i)}$ `torch.exp(tensor, out=None)`    \n",
    "\n",
    "$y_i=e^{(x_i)} -1$ `torch.expm1(tensor, out=None)`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7183, 1.0000, 1.0000],\n",
       "        [1.0000, 2.7183, 1.0000],\n",
       "        [1.0000, 1.0000, 2.7183]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.eye(3)\n",
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7183, 0.0000, 0.0000],\n",
       "        [0.0000, 1.7183, 0.0000],\n",
       "        [0.0000, 0.0000, 1.7183]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.expm1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较：\n",
    "`>`------`torch.gt(input, other, out=None)`  input>other\n",
    "\n",
    "`>=`-----`torch.ge(input, other, out=None)`  input>= other\n",
    "\n",
    "`<=`-----`torch.le(input, other, out=None)`  input=<other\n",
    "                                                         \n",
    "`<`-----`torch.lt(input, other, out=None)`  input<other\n",
    "                                                    \n",
    "`==`  `torch.eq(input, other, out=None)`  按成员进行等式操作，相同返回1 (理解为元素比较)\n",
    " \n",
    "`!=`   `torch.ne(input, other, out=None)`  input != other 不等于\n",
    "                                                         \n",
    "                                                         \n",
    "`torch.equal(tensor1, tensor2)` #如果tensor1和tensor2有相同的size和elements，则为true （理解为集合比较）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False],\n",
       "        [False,  True]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.equal(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.2500, 4.0000, 6.2500],\n",
       "        [0.2500, 1.0000, 2.2500, 4.0000]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0.5*torch.tensor([[2, 3, 4, 5],[1,2,3,4]])\n",
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.2500, 4.0000, 6.2500],\n",
       "        [0.2500, 1.0000, 2.2500, 4.0000]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(x,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元素幂运算与矩阵幂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 4.],\n",
       "        [4., 4.]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.ones((2,2),dtype=torch.float)\n",
    "x.matrix_power(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.pow(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   4,  27, 256])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3,4])**torch.tensor([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.clamp(input, min, max, out=None)\n",
    "\n",
    "将输入input张量每个元素的范围限制到区间 [min,max]，返回结果到一个新张量。\n",
    "\n",
    "input (Tensor) – 输入张量\n",
    "min (Number) – 限制范围下限\n",
    "max (Number) – 限制范围上限\n",
    "out (Tensor, optional) – 输出张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [3],\n",
      "        [6],\n",
      "        [6],\n",
      "        [1],\n",
      "        [6],\n",
      "        [4],\n",
      "        [0],\n",
      "        [2],\n",
      "        [4]])\n",
      "tensor([[3],\n",
      "        [3],\n",
      "        [5],\n",
      "        [5],\n",
      "        [3],\n",
      "        [5],\n",
      "        [4],\n",
      "        [3],\n",
      "        [3],\n",
      "        [4]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.randint(low=0,high=10,size=(10,1))\n",
    "print(a)\n",
    "a=torch.clamp(a,3,5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 10\n",
    "features = 25\n",
    "x = torch.rand((batch_size, features))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个样本的所有特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "print(x[0].shape)  # shape [25], this is same as doing x[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有样本的第一个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(x[:, 0].shape)  # shape [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三个样本的第一个至第10个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(x[2, 0:10].shape)  # shape: [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给指定元素（特定样本的特定特征）赋值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0, 0] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 花式索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Fancy Indexing\n",
    "x = torch.arange(10)\n",
    "indices = [2, 5, 8]\n",
    "print(x[indices])  # x[indices] = [2, 5, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二行第五列，以及第一行第一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7891, 0.5021])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand((3, 5))\n",
    "rows = torch.tensor([1, 0])\n",
    "cols = torch.tensor([4, 0])\n",
    "print(x[rows, cols])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "获取小于2或者大于8的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 9])\n"
     ]
    }
   ],
   "source": [
    "# More advanced indexing\n",
    "x = torch.arange(10)\n",
    "print(x[(x < 2) | (x > 8)])  # will be [0, 1, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取值为偶数的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x[x.remainder(2) == 0])  # remainder返回一个新的tensor,包含输入input张量每个元素的除法余数，余数与除数有相同的符号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y_i=\\left\\{\\begin{matrix}\n",
    " x_i& if \\quad x_i>5\\\\ \n",
    "x_{i}^{2} & otherwise\n",
    "\\end{matrix}\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "# Useful operations for indexing\n",
    "print(torch.where(x > 5, x, x * 2))  # gives [0, 2, 4, 6, 8, 10, 6, 7, 8, 9], all values x > 5 yield x, else x*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元素去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 0, 1, 2, 2, 3, 4]).unique()  # x = [0, 1, 2, 3, 4]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取维度，一共有几维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x.ndimension())  # The number of dimensions, in this case 1. if x.shape is 5x5x5 ndim would be 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "print(x.numel())  # The number of elements in x (in this case it's trivial because it's just a vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Reshaping    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================= #\n",
    "#                        Tensor Reshaping                       #\n",
    "# ============================================================= #\n",
    "\n",
    "x = torch.arange(9)\n",
    "\n",
    "# Let's say we want to reshape it to be 3x3\n",
    "x_3x3 = x.view(3, 3)\n",
    "\n",
    "# We can also do (view and reshape are very similar)\n",
    "# and the differences are in simple terms (I'm no expert at this),\n",
    "# is that view acts on contiguous tensors meaning if the\n",
    "# tensor is stored contiguously in memory or not, whereas\n",
    "# for reshape it doesn't matter because it will copy the\n",
    "# tensor to make it contiguously stored, which might come\n",
    "# with some performance loss.\n",
    "x_3x3 = x.reshape(3, 3)\n",
    "\n",
    "# If we for example do:\n",
    "y = x_3x3.t()\n",
    "print(\n",
    "    y.is_contiguous()\n",
    ")  # This will return False and if we try to use view now, it won't work!\n",
    "# y.view(9) would cause an error, reshape however won't\n",
    "\n",
    "# This is because in memory it was stored [0, 1, 2, ... 8], whereas now it's [0, 3, 6, 1, 4, 7, 2, 5, 8]\n",
    "# The jump is no longer 1 in memory for one element jump (matrices are stored as a contiguous block, and\n",
    "# using pointers to construct these matrices). This is a bit complicated and I need to explore this more\n",
    "# as well, at least you know it's a problem to be cautious of! A solution is to do the following\n",
    "print(y.contiguous().view(9))  # Calling .contiguous() before view and it works\n",
    "\n",
    "# Moving on to another operation, let's say we want to add two tensors dimensions togethor\n",
    "x1 = torch.rand(2, 5)\n",
    "x2 = torch.rand(2, 5)\n",
    "print(torch.cat((x1, x2), dim=0).shape)  # Shape: 4x5\n",
    "print(torch.cat((x1, x2), dim=1).shape)  # Shape 2x10\n",
    "\n",
    "# Let's say we want to unroll x1 into one long vector with 10 elements, we can do:\n",
    "z = x1.view(-1)  # And -1 will unroll everything\n",
    "\n",
    "# If we instead have an additional dimension and we wish to keep those as is we can do:\n",
    "batch = 64\n",
    "x = torch.rand((batch, 2, 5))\n",
    "z = x.view(\n",
    "    batch, -1\n",
    ")  # And z.shape would be 64x10, this is very useful stuff and is used all the time\n",
    "\n",
    "# Let's say we want to switch x axis so that instead of 64x2x5 we have 64x5x2\n",
    "# I.e we want dimension 0 to stay, dimension 1 to become dimension 2, dimension 2 to become dimension 1\n",
    "# Basically you tell permute where you want the new dimensions to be, torch.transpose is a special case\n",
    "# of permute (why?)\n",
    "z = x.permute(0, 2, 1)\n",
    "\n",
    "# Splits x last dimension into chunks of 2 (since 5 is not integer div by 2) the last dimension\n",
    "# will be smaller, so it will split it into two tensors: 64x2x3 and 64x2x2\n",
    "z = torch.chunk(x, chunks=2, dim=1)\n",
    "print(z[0].shape)\n",
    "print(z[1].shape)\n",
    "\n",
    "# Let's say we want to add an additional dimension\n",
    "x = torch.arange(\n",
    "    10\n",
    ")  # Shape is [10], let's say we want to add an additional so we have 1x10\n",
    "print(x.unsqueeze(0).shape)  # 1x10\n",
    "print(x.unsqueeze(1).shape)  # 10x1\n",
    "\n",
    "# Let's say we have x which is 1x1x10 and we want to remove a dim so we have 1x10\n",
    "x = torch.arange(10).unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "# Perhaps unsurprisingly\n",
    "z = x.squeeze(1)  # can also do .squeeze(0) both returns 1x10\n",
    "\n",
    "# That was some essential Tensor operations, hopefully you found it useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
